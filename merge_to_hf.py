import json
from pathlib import Path
import pandas as pd
import pyarrow as pa, pyarrow.parquet as pq

IN_ROOT = Path("out_data")
OUT_ROOT = IN_ROOT / "hf_dataset"
TRAIN_DIR = OUT_ROOT / "train"

COMPRESSION = "zstd"

def read_policy_df(pdir: Path):
    df = pd.read_parquet(pdir / "trajectories.parquet")
    agent = pdir.name

    # dtypes to help file size
    astypes = {
        "step": "int32", "reward":"int16", "sum":"int16", "done":"bool",
        "rng_seed":"int32", "H":"int16", "W":"int16"
    }
    for k,v in astypes.items():
        if k in df.columns:
            df[k] = df[k].astype(v)
    return df

def merge_episodes_jsonl(policy_dirs):
    out = OUT_ROOT / "episodes.jsonl"
    out.parent.mkdir(parents = True, exist_ok = True)
    with out.open("w", encoding="utf-8") as f_out:
        for d in policy_dirs:
            ep = d / "episodes.jsonl"
            if not ep.exists(): continue
            with ep.open("r", encoding="utf-8") as f_in:
                for line in f_in:
                    rec = json.loads(line)
                    f_out.write(json.dumps(rec, separators = (",",":")) + "\n")
    print(f"Merged episodes â†’ {out}")

def write_parquet(df: pd.DataFrame):
    TRAIN_DIR.mkdir(parents = True, exist_ok = True)
    df = df.sort_values(["episode_id","step"], kind = "mergesort").reset_index(drop = True)
    tbl = pa.Table.from_pandas(df, preserve_index = False)
    path = TRAIN_DIR / "train.parquet"
    pq.write_table(tbl, path, compression = COMPRESSION)
    print(f"Wrote {path} rows={len(df)}")

def main():
    # excluding high_pairs since its basically same as minimal area
    policy_dirs = [p for p in IN_ROOT.iterdir() if p.is_dir() and (p/"trajectories.parquet").exists() and p.name != "high_pairs_1k"]
    dfs = [read_policy_df(p) for p in policy_dirs]
    df = pd.concat(dfs, ignore_index = True)
    
    write_parquet(df)
    merge_episodes_jsonl(policy_dirs)
    print(f"Combined {len(df)} rows from {len(policy_dirs)} policies into {TRAIN_DIR}")

if __name__ == "__main__":
    main()